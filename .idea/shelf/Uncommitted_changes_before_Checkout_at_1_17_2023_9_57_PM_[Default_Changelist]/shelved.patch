Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nfrom imutils.video import VideoStream\r\nfrom imutils.perspective import four_point_transform\r\nfrom pytesseract import Output\r\n\r\nimport sqlite3\r\nimport cv2\r\nimport pytesseract\r\nimport argparse\r\nimport numpy as np\r\nimport imutils\r\nfrom imutils import paths\r\n\r\ndef cleanup_text(text):\r\n    # strip out non-ASCII text so we can draw the text on the image\r\n    # using OpenCV\r\n    return \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\r\n\r\n\r\ndef detect_blur_fft(image, size=60, thresh=10, vis=False):\r\n    # grab the dimensions of the image and use the dimensions to\r\n    # derive the center (x, y)-coordinates\r\n    (h, w) = image.shape\r\n    (cX, cY) = (int(w / 2.0), int(h / 2.0))\r\n    fft = np.fft.fft2(image)\r\n    fftShift = np.fft.fftshift(fft)\r\n    fftShift[cY - size:cY + size, cX - size:cX + size] = 0\r\n    fftShift = np.fft.ifftshift(fftShift)\r\n    recon = np.fft.ifft2(fftShift)\r\n    magnitude = 20 * np.log(np.abs(recon))\r\n    mean = np.mean(magnitude)\r\n    return (mean, mean <= thresh)\r\n\r\n\r\npytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\ACER\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\r\n# construct the argument parser and parse the arguments\r\nap = argparse.ArgumentParser()\r\nap.add_argument(\"-i\", \"--input\", required=True,\r\n                        help=\"path to input directory of images\")\r\nap.add_argument(\"-c\", \"--video\", type=int, default=-1,\r\n                    help=\"whether using video or image\")\r\nap.add_argument(\"-p\", \"--psm\", type=int, default=7,\r\n                help=\"default PSM mode for OCR'ing license plates\")\r\nap.add_argument(\"-d\", \"--debug\", type=int, default=-1,\r\n                help=\"whether or not to show additional visualizations\")\r\nargs = vars(ap.parse_args())\r\n\r\n\r\n\r\n\r\nprint(\"[INFO] opening video file...\")\r\n# Create a VideoCapture object and read from input file\r\ncap = cv2.VideoCapture(\"Dataset/Test/vid1r.mp4\")\r\n#Dataset/Test/image4.jpg\r\n\r\n\r\n#count frames:\r\nframeno=0\r\n# Check if camera opened successfully\r\nif (cap.isOpened() == False):\r\n    print(\"Error opening video file\")\r\n\r\n# Read until video is completed\r\nwhile (cap.isOpened()):\r\n\r\n\r\n    # Capture frame-by-frame\r\n    ret, orig = cap.read()\r\n    if ret == True:\r\n            frameno=frameno+1\r\n            frametext= \"Frame. no: ({:.4f})\"\r\n            frametext = frametext.format(frameno)\r\n            frame = imutils.resize(orig, width=600)\r\n            ratio = orig.shape[1] / float(frame.shape[1])\r\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n            (mean, blurry) = detect_blur_fft(gray, thresh=15)\r\n            # draw whether or not the frame is blurry\r\n            color = (0, 0, 255) if blurry else (0, 255, 0)\r\n            text = \"Blurry ({:.4f})\" if blurry else \"Not Blurry ({:.4f})\"\r\n            text = text.format(mean)\r\n            cv2.putText(frame, text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,\r\n                        0.7, color, 2)\r\n            cv2.putText(frame, frametext, (250, 25), cv2.FONT_HERSHEY_SIMPLEX,\r\n                        0.7, color, 2)\r\n\r\n            if not blurry:\r\n\r\n                bfilter = cv2.bilateralFilter(gray, 11, 17, 17)  # Noise reduction\r\n                edged = cv2.Canny(bfilter, 30, 200)  # Edge detection\r\n\r\n                keypoints = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\r\n                contours = imutils.grab_contours(keypoints)\r\n                contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\r\n                location = None\r\n                for contour in contours:\r\n                    approx = cv2.approxPolyDP(contour, 10, True)\r\n                    if len(approx) == 4:\r\n                        location = approx\r\n                        break\r\n                mask = np.zeros(gray.shape, np.uint8)\r\n                try:\r\n                    new_image = cv2.drawContours(mask, [location], 0, 255, -1)\r\n                    new_image = cv2.bitwise_and(gray, gray, mask=mask)\r\n                    (x, y) = np.where(mask == 255)\r\n                    (x1, y1) = (np.min(x), np.min(y))\r\n                    (x2, y2) = (np.max(x), np.max(y))\r\n                    cropped_image = gray[x1:x2 + 1, y1:y2 + 1]\r\n                    cropped_image = cv2.bilateralFilter(cropped_image, 11, 17, 17)  # Noise reduction\r\n                    kernel = np.array([[0, -1, 0],\r\n                                       [-1, 5, -1],\r\n                                       [0, -1, 0]])\r\n                    cropped_image = cv2.filter2D(cropped_image, ddepth=-1, kernel=kernel)\r\n\r\n                    '''\r\n                    #skew correction:\r\n                    gray = cv2.bitwise_not(cropped_image)\r\n                    # threshold the image, setting all foreground pixels to\r\n                    # 255 and all background pixels to 0\r\n                    thresh = cv2.threshold(gray, 0, 255,\r\n                                           cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\r\n    \r\n                    coords = np.column_stack(np.where(thresh > 0))\r\n                    angle = cv2.minAreaRect(coords)[-1]\r\n                    # the `cv2.minAreaRect` function returns values in the\r\n                    # range [-90, 0); as the rectangle rotates clockwise the\r\n                    # returned angle trends to 0 -- in this special case we\r\n                    # need to add 90 degrees to the angle\r\n                    if angle < -45:\r\n                        angle = -(90 + angle)\r\n                    # otherwise, just take the inverse of the angle to make\r\n                    # it positive\r\n                    else:\r\n                        angle = -angle\r\n                        # rotate the image to deskew it\r\n                        (h, w) = image.shape[:2]\r\n                        center = (w // 2, h // 2)\r\n                        M = cv2.getRotationMatrix2D(center, angle, 1.0)\r\n                        rotated = cv2.warpAffine(cropped_image, M, (w, h),\r\n                                                 flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\r\n                        # draw the correction angle on the image so we can validate it\r\n                        cv2.putText(rotated, \"Angle: {:.2f} degrees\".format(angle),\r\n                                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\r\n                        # show the output image\r\n                        print(\"[INFO] angle: {:.3f}\".format(angle))\r\n                        cv2.imshow(\"Input\", cropped_image)\r\n                        cv2.waitKey(0)\r\n                        cv2.imshow(\"Rotated\", rotated)\r\n                        cv2.waitKey(0)\r\n                    '''\r\n\r\n                    alphanumeric = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \"\r\n                    options = \"-c tessedit_char_whitelist={}\".format(alphanumeric)\r\n                    options += \" --psm {}\".format(13)  # set\r\n                    kernel = np.ones((1, 1), np.uint8)\r\n\r\n                    reader = pytesseract.image_to_string(cropped_image, config=options)\r\n                    result = reader\r\n                    text = cleanup_text(result)\r\n                    font = cv2.FONT_HERSHEY_SIMPLEX\r\n                    res = cv2.putText(frame, text=text, org=(approx[0][0][0], approx[1][0][1] + 60), fontFace=font,\r\n                                      fontScale=1,\r\n                                      color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)\r\n                    res = cv2.rectangle(frame, tuple(approx[0][0]), tuple(approx[2][0]), (0, 255, 0), 3)\r\n                    print(text)\r\n                    cv2.imshow('Frame', frame)\r\n                except:\r\n                    print(\"error\")\r\n\r\n            # Press Q on keyboard to exit\r\n            if cv2.waitKey(40) & 0xFF == ord('q'):\r\n                break\r\n    else:\r\n                break\r\n\r\n# When everything done, release\r\n# the video capture object\r\ncap.release()\r\n\r\n# Closes all the frames\r\ncv2.destroyAllWindows()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- main.py	(revision 717b74901b3981271e6dfa6183be0dcf87453cea)
+++ main.py	(date 1673971556995)
@@ -1,9 +1,4 @@
 
-from imutils.video import VideoStream
-from imutils.perspective import four_point_transform
-from pytesseract import Output
-
-import sqlite3
 import cv2
 import pytesseract
 import argparse
@@ -16,165 +11,49 @@
     # using OpenCV
     return "".join([c if ord(c) < 128 else "" for c in text]).strip()
 
-
-def detect_blur_fft(image, size=60, thresh=10, vis=False):
-    # grab the dimensions of the image and use the dimensions to
-    # derive the center (x, y)-coordinates
-    (h, w) = image.shape
-    (cX, cY) = (int(w / 2.0), int(h / 2.0))
-    fft = np.fft.fft2(image)
-    fftShift = np.fft.fftshift(fft)
-    fftShift[cY - size:cY + size, cX - size:cX + size] = 0
-    fftShift = np.fft.ifftshift(fftShift)
-    recon = np.fft.ifft2(fftShift)
-    magnitude = 20 * np.log(np.abs(recon))
-    mean = np.mean(magnitude)
-    return (mean, mean <= thresh)
-
-
 pytesseract.pytesseract.tesseract_cmd = r'C:\Users\ACER\AppData\Local\Tesseract-OCR\tesseract.exe'
-# construct the argument parser and parse the arguments
-ap = argparse.ArgumentParser()
-ap.add_argument("-i", "--input", required=True,
-                        help="path to input directory of images")
-ap.add_argument("-c", "--video", type=int, default=-1,
-                    help="whether using video or image")
-ap.add_argument("-p", "--psm", type=int, default=7,
-                help="default PSM mode for OCR'ing license plates")
-ap.add_argument("-d", "--debug", type=int, default=-1,
-                help="whether or not to show additional visualizations")
-args = vars(ap.parse_args())
-
-
-
-
-print("[INFO] opening video file...")
-# Create a VideoCapture object and read from input file
-cap = cv2.VideoCapture("Dataset/Test/vid1r.mp4")
-#Dataset/Test/image4.jpg
-
-
-#count frames:
-frameno=0
-# Check if camera opened successfully
-if (cap.isOpened() == False):
-    print("Error opening video file")
-
-# Read until video is completed
-while (cap.isOpened()):
-
-
-    # Capture frame-by-frame
-    ret, orig = cap.read()
-    if ret == True:
-            frameno=frameno+1
-            frametext= "Frame. no: ({:.4f})"
-            frametext = frametext.format(frameno)
+def readtxt(orig):
             frame = imutils.resize(orig, width=600)
-            ratio = orig.shape[1] / float(frame.shape[1])
             gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
-            (mean, blurry) = detect_blur_fft(gray, thresh=15)
-            # draw whether or not the frame is blurry
-            color = (0, 0, 255) if blurry else (0, 255, 0)
-            text = "Blurry ({:.4f})" if blurry else "Not Blurry ({:.4f})"
-            text = text.format(mean)
-            cv2.putText(frame, text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,
-                        0.7, color, 2)
-            cv2.putText(frame, frametext, (250, 25), cv2.FONT_HERSHEY_SIMPLEX,
-                        0.7, color, 2)
-
-            if not blurry:
 
-                bfilter = cv2.bilateralFilter(gray, 11, 17, 17)  # Noise reduction
-                edged = cv2.Canny(bfilter, 30, 200)  # Edge detection
+            bfilter = cv2.bilateralFilter(gray, 11, 17, 17)  # Noise reduction
+            edged = cv2.Canny(bfilter, 30, 200)  # Edge detection
 
-                keypoints = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
-                contours = imutils.grab_contours(keypoints)
-                contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]
-                location = None
-                for contour in contours:
-                    approx = cv2.approxPolyDP(contour, 10, True)
-                    if len(approx) == 4:
-                        location = approx
-                        break
-                mask = np.zeros(gray.shape, np.uint8)
-                try:
-                    new_image = cv2.drawContours(mask, [location], 0, 255, -1)
-                    new_image = cv2.bitwise_and(gray, gray, mask=mask)
-                    (x, y) = np.where(mask == 255)
-                    (x1, y1) = (np.min(x), np.min(y))
-                    (x2, y2) = (np.max(x), np.max(y))
-                    cropped_image = gray[x1:x2 + 1, y1:y2 + 1]
-                    cropped_image = cv2.bilateralFilter(cropped_image, 11, 17, 17)  # Noise reduction
-                    kernel = np.array([[0, -1, 0],
-                                       [-1, 5, -1],
-                                       [0, -1, 0]])
-                    cropped_image = cv2.filter2D(cropped_image, ddepth=-1, kernel=kernel)
-
-                    '''
-                    #skew correction:
-                    gray = cv2.bitwise_not(cropped_image)
-                    # threshold the image, setting all foreground pixels to
-                    # 255 and all background pixels to 0
-                    thresh = cv2.threshold(gray, 0, 255,
-                                           cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]
-    
-                    coords = np.column_stack(np.where(thresh > 0))
-                    angle = cv2.minAreaRect(coords)[-1]
-                    # the `cv2.minAreaRect` function returns values in the
-                    # range [-90, 0); as the rectangle rotates clockwise the
-                    # returned angle trends to 0 -- in this special case we
-                    # need to add 90 degrees to the angle
-                    if angle < -45:
-                        angle = -(90 + angle)
-                    # otherwise, just take the inverse of the angle to make
-                    # it positive
-                    else:
-                        angle = -angle
-                        # rotate the image to deskew it
-                        (h, w) = image.shape[:2]
-                        center = (w // 2, h // 2)
-                        M = cv2.getRotationMatrix2D(center, angle, 1.0)
-                        rotated = cv2.warpAffine(cropped_image, M, (w, h),
-                                                 flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
-                        # draw the correction angle on the image so we can validate it
-                        cv2.putText(rotated, "Angle: {:.2f} degrees".format(angle),
-                                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
-                        # show the output image
-                        print("[INFO] angle: {:.3f}".format(angle))
-                        cv2.imshow("Input", cropped_image)
-                        cv2.waitKey(0)
-                        cv2.imshow("Rotated", rotated)
-                        cv2.waitKey(0)
-                    '''
-
-                    alphanumeric = "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 "
-                    options = "-c tessedit_char_whitelist={}".format(alphanumeric)
-                    options += " --psm {}".format(13)  # set
-                    kernel = np.ones((1, 1), np.uint8)
-
-                    reader = pytesseract.image_to_string(cropped_image, config=options)
-                    result = reader
-                    text = cleanup_text(result)
-                    font = cv2.FONT_HERSHEY_SIMPLEX
-                    res = cv2.putText(frame, text=text, org=(approx[0][0][0], approx[1][0][1] + 60), fontFace=font,
-                                      fontScale=1,
-                                      color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)
-                    res = cv2.rectangle(frame, tuple(approx[0][0]), tuple(approx[2][0]), (0, 255, 0), 3)
-                    print(text)
-                    cv2.imshow('Frame', frame)
-                except:
-                    print("error")
-
-            # Press Q on keyboard to exit
-            if cv2.waitKey(40) & 0xFF == ord('q'):
-                break
-    else:
-                break
-
-# When everything done, release
-# the video capture object
-cap.release()
-
-# Closes all the frames
-cv2.destroyAllWindows()
\ No newline at end of file
+            keypoints = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
+            contours = imutils.grab_contours(keypoints)
+            contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]
+            location = None
+            for contour in contours:
+                approx = cv2.approxPolyDP(contour, 10, True)
+                if len(approx) == 4:
+                    location = approx
+                    break
+            mask = np.zeros(gray.shape, np.uint8)
+            try:
+                new_image = cv2.drawContours(mask, [location], 0, 255, -1)
+                new_image = cv2.bitwise_and(gray, gray, mask=mask)
+                (x, y) = np.where(mask == 255)
+                (x1, y1) = (np.min(x), np.min(y))
+                (x2, y2) = (np.max(x), np.max(y))
+                cropped_image = gray[x1:x2 + 1, y1:y2 + 1]
+                cropped_image = cv2.bilateralFilter(cropped_image, 11, 17, 17)  # Noise reduction
+                kernel = np.array([[0, -1, 0],
+                                   [-1, 5, -1],
+                                   [0, -1, 0]])
+                cropped_image = cv2.filter2D(cropped_image, ddepth=-1, kernel=kernel)
+                alphanumeric = "ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 "
+                options = "-c tessedit_char_whitelist={}".format(alphanumeric)
+                options += " --psm {}".format(13)  # set
+                kernel = np.ones((1, 1), np.uint8)
+                reader = pytesseract.image_to_string(cropped_image, config=options)
+                result = reader
+                text = cleanup_text(result)
+                font = cv2.FONT_HERSHEY_SIMPLEX
+                res = cv2.putText(frame, text=text, org=(approx[0][0][0], approx[1][0][1] + 80), fontFace=font,
+                                  fontScale=1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)
+                res = cv2.rectangle(frame, tuple(approx[0][0]), tuple(approx[2][0]), (0, 255, 0), 3)
+                print(text)
+                return res
+            except:
+                res=frame
+                return frame
Index: sys.py
===================================================================
--- sys.py	(date 1673889506249)
+++ sys.py	(date 1673889506249)
@@ -0,0 +1,73 @@
+from PyQt5 import QtGui
+import main
+from PyQt5.QtWidgets import QWidget, QApplication, QLabel, QVBoxLayout
+from PyQt5.QtGui import QPixmap
+import sys
+import cv2
+from PyQt5.QtCore import pyqtSignal, pyqtSlot, Qt, QThread
+import numpy as np
+
+
+class VideoThread(QThread):
+    change_pixmap_signal = pyqtSignal(np.ndarray)
+
+    def run(self):
+        # capture from web cam
+        cap = cv2.VideoCapture("Dataset/Test/vid1r.mp4")
+        while True:
+            ret, cv_img = cap.read()
+            cv_img = main.readtxt(cv_img)
+            if ret:
+                self.change_pixmap_signal.emit(cv_img)
+
+
+
+
+
+class App(QWidget):
+    def __init__(self):
+        super().__init__()
+        self.setWindowTitle("Qt live label demo")
+        self.disply_width = 640
+        self.display_height = 480
+        # create the label that holds the image
+        self.image_label = QLabel(self)
+        self.image_label.resize(self.disply_width, self.display_height)
+        # create a text label
+        self.textLabel = QLabel('Webcam')
+
+        # create a vertical box layout and add the two labels
+        vbox = QVBoxLayout()
+        vbox.addWidget(self.image_label)
+        vbox.addWidget(self.textLabel)
+        # set the vbox layout as the widgets layout
+        self.setLayout(vbox)
+
+        # create the video capture thread
+        self.thread = VideoThread()
+        # connect its signal to the update_image slot
+        self.thread.change_pixmap_signal.connect(self.update_image)
+        # start the thread
+        self.thread.start()
+
+    @pyqtSlot(np.ndarray)
+    def update_image(self, cv_img):
+        """Updates the image_label with a new opencv image"""
+        qt_img = self.convert_cv_qt(cv_img)
+        self.image_label.setPixmap(qt_img)
+
+    def convert_cv_qt(self, cv_img):
+        """Convert from an opencv image to QPixmap"""
+        rgb_image = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)
+        h, w, ch = rgb_image.shape
+        bytes_per_line = ch * w
+        convert_to_Qt_format = QtGui.QImage(rgb_image.data, w, h, bytes_per_line, QtGui.QImage.Format_RGB888)
+        p = convert_to_Qt_format.scaled(self.disply_width, self.display_height, Qt.KeepAspectRatio)
+        return QPixmap.fromImage(p)
+
+
+if __name__ == "__main__":
+    app = QApplication(sys.argv)
+    a = App()
+    a.show()
+    sys.exit(app.exec_())
\ No newline at end of file
